[{"content":"Problem Consider your product is going through a big overhaul and re-architecturing where the existing microservices might be broken into multiple ones or merged to single ones, there will be a big jumble of mess of routes at the ingress level. This will make transitioning a bit hard as using canary based routing will gradually become complicated. So to simplify this canary setup, we take the routing matter into our own hands. If you can use Consul and it works right for you, that might be the better option as Consul is more mature and production ready. You will be less likely to break things. Solution Architecture proposed: Implementation Its Almost same as auth implementation but will require a bit of scripting as we are not just checking status code to return 401 or to allow the traffic. This can be easily achieved with configuration snippet annotation in ingress-nginx. We write a configuration snippet that sends request to the server that is responsible for getting the route information from the request body. Which more or less looks like one below: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: docs-service annotations: nginx.ingress.kubernetes.io/configuration-snippet: | access_by_lua_block { local cjson = require(\u0026#34;cjson\u0026#34;) local http = require \u0026#34;resty.http\u0026#34; local default_backend=\u0026#34;docs1\u0026#34; local httpc = http.new() local res, err = httpc:request_uri(\u0026#34;http://authserver.ingress-nginx-helpers.svc.cluster.local:5000/find-backend\u0026#34;, { method = \u0026#34;POST\u0026#34;, body = cjson.encode({ real_url = ngx.var.request_uri }), headers = { [\u0026#34;Content-Type\u0026#34;] = \u0026#34;application/json\u0026#34;, [\u0026#34;Token\u0026#34;] = ngx.req.get_headers()[\u0026#39;token\u0026#39;], [\u0026#34;cookie\u0026#34;] = ngx.req.get_headers()[\u0026#39;cookie\u0026#39;], [\u0026#34;x-api-key\u0026#34;] = ngx.req.get_headers()[\u0026#39;x-api-key\u0026#39;] } }) local response, decode_err = cjson.decode(res.body) if response then ngx.var.proxy_upstream_name = ngx.var.namespace .. \u0026#34;-\u0026#34; .. response.backend .. ngx.var.service_port end ngx.ctx.balancer=nil } spec: ingressClassName: nginx rules: - host: hello.world.com http: paths: - backend: service: name: docs1 port: number: 5000 path: /api/v1/docs1/docs/custom/ pathType: Exact This will match for path /api/v1/docs1/docs/custom/ and send request to the server to get backend information. Which in turn will be used to populate proxy_upstream_name Don\u0026rsquo;t forget to set ngx.ctx.balancer to nil at last. Read here Enhancement This for POC looks good but for real world application we try to make it as frictionless as possible. Creating a ingress-nginx plugin We create a plugin that does all of this and a flag that will determine weather to run this plugin or not for each ingress. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 apiVersion: v1 kind: ConfigMap metadata: name: ngx-custom-script namespace: ingress-nginx data: main.lua: | local ngx = ngx local _M = {} local cjson = require(\u0026#34;cjson\u0026#34;) local http = require \u0026#34;resty.http\u0026#34; function _M.rewrite() if not (ngx.var.dynamic_plugin == \u0026#34;on\u0026#34;) then return end local final_backend=\u0026#34;eevee\u0026#34; local httpc = http.new() local res, err = httpc:request_uri(\u0026#34;http://authserver.ingress-nginx-helpers.svc.cluster.local:5000/find-backend\u0026#34;, { method = \u0026#34;POST\u0026#34;, body = cjson.encode({ real_url = ngx.var.request_uri }), headers = { [\u0026#34;Content-Type\u0026#34;] = \u0026#34;application/json\u0026#34;, [\u0026#34;Token\u0026#34;] = ngx.req.get_headers()[\u0026#39;token\u0026#39;], [\u0026#34;cookie\u0026#34;] = ngx.req.get_headers()[\u0026#39;cookie\u0026#39;], [\u0026#34;x-api-key\u0026#34;] = ngx.req.get_headers()[\u0026#39;x-api-key\u0026#39;] } }) local response, decode_err = cjson.decode(res.body) if response then final_backend=response.backend end ngx.var.proxy_upstream_name = ngx.var.namespace .. \u0026#34;-\u0026#34; .. final_backend .. \u0026#34;-\u0026#34; .. ngx.var.service_port ngx.ctx.balancer=nil end return _M As we can see there is a flag up top dynamic_plugin that will be set in each ingress as a configurations snippet to trigger the execution of this script in each request. The plugin code will be written in config-map and will be mounted to ingress controller kubernetes pod as a lua file in the plugins directory. Mounting the plugin We add the volume mount in out controller deployment by adding following lines in the right place: 1 2 3 4 5 6 7 8 9 10 volumes: - configMap: defaultMode: 420 name: ngx-custom-script name: lua-scripts-volume volumeMounts: - mountPath: /etc/nginx/lua/plugins/modify_request/ name: lua-scripts-volume readOnly: true More about creating a nginx lua plugin here. Running the plugin Create a new ingress, omitting the lua part and adding a configuration snippet annotation that sets the flag written in the plugin lua. The annotations will be like: 1 2 nginx.ingress.kubernetes.io/configuration-snippet: | set $dynamic_plugin \u0026#34;on\u0026#34;; Now each request sent to the ingress with this variable set will have its route dynamically created as per your requirement. The BE-finder server This is the backbone of your ingress, so should be as fast as possible. Thus it should be written in efficient languages like Rust, Golang or C/C++. The architecture proposed will first check for the required information in the redis and does not bother to send next request to database as we won\u0026rsquo;t have enough time and are okay with a few leakage traffic. The cache miss will trigger a goroutine that fetches the information from database for the next request to pick up. Conclusion We do not need complicated setup with service meshes to get intelligent routing. Performance will be better than service mesh based solution. Easy to setup and use. ","date":"2024-07-10T00:44:36+05:45","permalink":"https://ankitp.com.np/p/request-body-aware-dynamic-routing-in-ingress-nginx-using-custom-plugins./","title":"Request body aware dynamic routing in ingress nginx using custom plugins."},{"content":"How we made debugging on cloud easier without costing a fortune. Preface Any organization will require good and flexible environment for every developer to test their code before merging it to the main branch.\nThis is normally not required for small-medium monolith application but if the application is too big or consists of a lot microservices and we want to test integration between mutliple microservices. This does not occur often but occurs when you have a feature to implement that will span changes across a lot of microservices. There are other method to test these like writing unit tests for each api of microservice so that it adhers to certain schema and changing the schema as per the changes done and the expected value.\nBut before any release the end to end test will anyhow be necessary.\nThe Problem All the pushes makes their code live on single shared environment (Testing Env) clashing feature specific tests. If someone pushes faulty code, making a microservice halt, all the other ongoing test will be affected. This will be a major setback if there is some feature that requires quite a bit of time to test one feature. Resulting in Increased time for almost every feature release. Which sometime meant lost first mover advantage. The Solution Separate resource for each developer or shared resources among a team This method will work but will be expensive as there will be constant resource consumption if any team forgets to release their resources, unless we think about premption of unused resources which will be difficult to implement or require cloud platform specific implementation or some terraform script to do so. This will also allow team to use the resource allocated to them for something else then work. Its okay if you have enough resources as developers also need some breathing room to test new stuffs but will be difficult to maintain if you already have thousands of dollars of bill from your cloud provider every week. Automatic Creation of required resources and their premption on inactivity the k8s way. This method was most suitable in our case as separating the resources for different teams to test means less traffic on existing shared Testing Env. This made such that we can decrease the resource footprint of the existing shared env. We settled in the conclusion of creating 12 static namespaces that will house our entire product\u0026rsquo;s deployment. And making a mapping sheet to map teams to those namespaces. Our company has unfettered mania of naming things after things in pokemon so we decided to go with the same flow to name our namespaces and environment by pokemon teams, 12 of them. Setup Minimal permission Make it such that people have view access to the 12 namespaces defined so they can realtime stream container logs or look at the exact config of the deployment or pod. Mapping people to namespaces We wanted to make this part as effortless as possible for the developers along with decreasing our effort of managing the pipeline configuration. So we settled with Google Sheet. We made a sheet with 3 colums: Slot, Usernames, Purpose. The first had list of 12 namespaces we created, second would house github usernames of guys currently claiming that space separated by comma, and the last would be some sort of remarks to indicate why was the slot occupied by that person. The Workflow Someone pushes code to our backend repos. Github workflow is triggred. We used google\u0026rsquo;s gspread client and inlined python script to access the content of the namespace sheet. Installs the backend helm package to the namespace found from the sheet. Below is the actual step in github workflows that we used to interface gsheet with github workflow: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 shell: bash run: |- pip3 install gspread==5.12 oauth2client SLOT_INFO=$(python3 -c \u0026#34; import gspread; from oauth2client.service_account import ServiceAccountCredentials; scope = [ \u0026#39;https://www.googleapis.com/auth/spreadsheets\u0026#39;, \u0026#39;https://www.googleapis.com/auth/drive.file\u0026#39;, \u0026#39;https://www.googleapis.com/auth/drive\u0026#39; ]; creds = ServiceAccountCredentials.from_json_keyfile_name(\u0026#39;$GOOGLE_APPLICATION_CREDENTIALS\u0026#39;, scope); client = gspread.authorize(creds); datas = client.open(\u0026#39;$SHEET_NAME\u0026#39;).sheet1.get_all_records(); print(f\u0026#39;slot mapping data: {datas}\u0026#39;); slot_of_interest = [val for val in datas if \u0026#39;$TRIGGER_EMAIL\u0026#39; in list(map(lambda e: e.strip(), val[\u0026#39;Emails\u0026#39;].split(\u0026#39;,\u0026#39;))) and \u0026#39;$TRIGGER_EMAIL\u0026#39; ]; [print(f\u0026#39;\u0026#39;\u0026#39;{data[\u0026#39;Slot\u0026#39;]}::{data[\u0026#39;Emails\u0026#39;]}\u0026#39;\u0026#39;\u0026#39;)for data in slot_of_interest ]; \u0026#34; ) echo $SLOT_INFO SLOT_INFO=$(echo \u0026#34;$SLOT_INFO\u0026#34; | grep -m 1 -E \u0026#39;::\u0026#39; ) || : if [[ \u0026#34;$SLOT_INFO\u0026#34; == \u0026#34;\u0026#34; ]]; then echo \u0026#34;User not allocated on any namepace. Please update the sheet at https://tinyurl.com/docsumo-namespace with your username ($TRIGGER_EMAIL).\u0026#34; exit 1 fi echo \u0026#34;found slot details of current user. $SLOT_INFO\u0026#34; KUBE_NAMESPACE=$(echo $SLOT_INFO | awk -F\u0026#34;::\u0026#34; \u0026#39;{print $1}\u0026#39; ) IFS=$\u0026#39;,\u0026#39; read -ra EMAILS \u0026lt;\u0026lt;\u0026lt; $(echo $SLOT_INFO | awk -F\u0026#34;::\u0026#34; \u0026#39;{print $2}\u0026#39; ) HOST=${KUBE_NAMESPACE}.docsumo.com echo \u0026#34;HOST=$HOST\u0026#34; \u0026gt;\u0026gt; $GITHUB_ENV echo \u0026#34;KUBE_NAMESPACE=$KUBE_NAMESPACE\u0026#34; \u0026gt;\u0026gt; $GITHUB_ENV echo \u0026#34;final env values; HOST; $HOST, emails; ${EMAILS[@]}, KUBE_NAMESPACE; $KUBE_NAMESPACE\u0026#34; Each namespace will have a cronjob monitoring controller logs and if no request is coming for the last 1 hour, we delete all the resource consuming resources in that namespace, That will include all the deployments and statefulsets. The Premption\nThe premption of resource was handled by a cronjob installed in each namespace that ingested gcloud ingress logs to see if someone was using the namespace. If there is no request logs in the last 1 hour, we delete all the resources of that namespace by doing helm uninstall on all the installed charts. Below is the exact cronjob that we used for this purpose. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 apiVersion: batch/v1 kind: CronJob metadata: name: delete-on-inactivity namespace: {{ .Release.Namespace }} spec: schedule: \u0026#34;0 * * * *\u0026#34; jobTemplate: spec: template: spec: serviceAccount: pod-manager containers: - name: log-checker image: {{ .Values.cronjobimage.repository }}:{{ .Values.cronjobimage.tag }} args: - /bin/bash - -c - | RELEASES=$(helm ls --short -n {{ .Release.Namespace }} | grep -vE \u0026#39;roles\u0026#39;) if [[ ! \u0026#34;$RELEASES\u0026#34; ]]; then echo \u0026#34;Nothing found that can be deleted, exiting.\u0026#34;; exit 0; fi echo $LOG_READER_CREDS | base64 -d \u0026gt; log_reader.json gcloud auth activate-service-account --key-file=log_reader.json gcloud config set project {{ .Values.project_id }} OFFICE_TIME=(3 14) if [[ $(date -u +%-H) -le ${OFFICE_TIME[1]} \u0026amp;\u0026amp; $(date -u +%-H) -ge ${OFFICE_TIME[0]} ]]; then echo \u0026#34;Its office time..\u0026#34; TIME_PERIOD=`date -u -d\u0026#39;2 hour ago\u0026#39; +%Y-%m-%dT%TZ` else TIME_PERIOD=`date -u -d\u0026#39;1 hour ago\u0026#39; +%Y-%m-%dT%TZ` fi gcloud logging read \u0026#34;timestamp \u0026gt;= \\\u0026#34;${TIME_PERIOD}\\\u0026#34; AND resource.type=\\\u0026#34;k8s_container\\\u0026#34; AND resource.labels.cluster_name=\\\u0026#34;testing-cluster\\\u0026#34; AND resource.labels.container_name=\\\u0026#34;controller\\\u0026#34; AND resource.labels.namespace_name=\\\u0026#34;ingress-nginx\\\u0026#34; AND jsonPayload.proxyUpstreamName~=\\\u0026#34;{{ .Release.Namespace }}\\\u0026#34;\u0026#34; --limit 10 --format=json \u0026gt; output.json count=$(jq \u0026#39;. | length\u0026#39; output.json); if [[ $count -eq 0 ]]; then echo \u0026#34;no Logs found in namespace \u0026#39;{{ .Release.Namespace }}\u0026#39; for last one hour, deleting all the resources.\u0026#34; helm ls --short -n {{ .Release.Namespace }} | grep -vE \u0026#39;roles|config\u0026#39; | xargs -I {} helm uninstall {} -n {{ .Release.Namespace }} --debug else echo \u0026#34;logs found on last one hour, not deleting the releases\u0026#34; fi env: - name: LOG_READER_CREDS valueFrom: secretKeyRef: name: google-app-credential-logging key: GOOGLE_APPLICATION_CREDENTIALS_LOGGING restartPolicy: OnFailure Blockers faced in the way We are using external secret operator to sync our backend secrets with GCP\u0026rsquo;s secret manager which when deleted in bulk was failing saying it failed to call some internal webhook. Probably overloading the internal webhook deployment of the operator. We then decided to not delete configmaps and secrets from namespaces as they anyways would not take much resources. Possible enhancements The next thing we can do is debug totally in cloud. I have another article written about how we can actually remote debug our python code in vscode. Here\nWe setup each deployment to run debugpy instead of normal gunicorn and with a little bit of firewall setup and ip whitelisting, we can actually connect to remote debugger. This allows for debugging in the actual hardware that the production server runs on which helps debug cloud specific issues in some instances\nConclusion First shield of defence: make each and every microservice autonomous so the need of integration tests are rare. But if your domain does not allow you to do that then this way of making separate test environment would be cheap and effective in the long term of your startup journey. In the end, each developer having access to a vm or multiple vagrant instances would come into picture but when the money and resources are limited, this approach is also not bad.\n","date":"2024-06-23T09:30:05+05:45","permalink":"https://ankitp.com.np/p/managing-on-demand-ephemeral-microservice-development-environment-in-a-single-k8s-cluster./","title":"Managing on demand ephemeral microservice development environment in a single k8s cluster."},{"content":"Debugging python inside of a docker container Premises Lets say you are working on some legacy python project which has very obscure and obsolete dependencies and you now want to debug it. First lets pray to god that, this day does not come to anybody but if you are working on some project without standard practices on package management, this kind of situation is not uncommon.\nThe Problem Making a local dev envs.\nEven though pyenv and poetry to name a few is able to install different versions of python along with old deps, what if the packages are so old even pip does not have them. A few old python packages do delete old version from pipy instead of yanking them that will cause the old programs to even stop running in the near future. The next problem is bloat if you have multiple of these shitty projects then you would not want your whole system to endure the cost of multiple old and obsolete python versions in the long run. Lets admit it nobody remembers to uninstall the things that we installed just to make things to working, instead we leave it in our system till we get out of space or some other things break due to that. The latter case will be the most troublesome because it might take another full day just to know the reason why your new program is not working and that the problem was you having an old version of something else. The solution My solution to this problem is to debug inside of a container. You replicate the environment that your system was running on, inside of a container.\nThis is a breeze if you already had your program containerized.\nGet the container that has been running this whole time in production. Either docker pull or form sysadmin. Here is one of the answer on how to load the image to your local docker instance if you don\u0026rsquo;t have access to docker pull image: https://stackoverflow.com/questions/37905763/how-do-i-download-docker-images-without-using-the-pull-command But if you had not done so before, you need answers to following things from your sysadmin or anyone who has access to the running system:\nWhat is the OS? if Its Windows, Goodluck on that. Go the usual pyenv/poetry route. The output of pip freeze and python version. Get hands on requirements.txt. Now containerize your application.\nMake a simple docker image having following steps: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ## The name and versions should be the one from your production system. ## It should be available in dockerhub as they normally have older images too. FROM debian:\u0026lt;tag\u0026gt; ## Install the required libraries. apt is for debian. RUN apt update \u0026amp;\u0026amp; apt install ..... ## Install debugpy to initialize remote debugging session. RUN pip install debugpy COPY ./requirements.txt /requirements.txt RUN pip install -r requirements.txt ## Some other pip installs you have some custom python packages to install. ## This so that you can get shell access to the docker without doing docker exec anything CMD [\u0026#34;bash\u0026#34;] If you have a bit of complex program requirements like requiring redis or some other things then you can use a docker-compose.yaml file. This was the case in my case. Feel free to take reference from dockercompose below:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 services: dev-container: ## I have this because I have access to do docker pull. ## If you have to build from dockerfile, you can give this any name, ## then uncomment the build part below image: us-docker.pkg.dev/\u0026lt;project_id\u0026gt;/us.gcr.io/\u0026lt;image_name\u0026gt;:\u0026lt;image_tag\u0026gt; # build: # context: . # dockerfile: Dockerfile # args: # - GITLAB_DEPLOY_TOKEN=\u0026lt;GITLAB_TOKEN\u0026gt; ## I needed this because I had custom old package to install from gitlab\u0026#39;s package registry. volumes: - ./app:/app - ./service_account.json:/srv/key.json ## this was required to access google specific resource inside the program env_file: - \u0026#34;.env\u0026#34; environment: - GOOGLE_APPLICATION_CREDENTIAL_PATH=/srv/key.json - FLASK_APP=app.app:app ## uncomment this instead of network_mode : host if you have the ports inside the container occupied. # ports: # - 8080:8080 network_mode: \u0026#34;host\u0026#34; stdin_open: true tty: true command: [\u0026#34;bash\u0026#34;] ## my program required redis too. redis-master: image: redis:latest # ports: # - \u0026#34;6379:6379\u0026#34; command: redis-server --appendonly yes --requirepass 123456789 network_mode: \u0026#34;host\u0026#34; env_file: - \u0026#34;.env\u0026#34; Debugpy to rescue Debugpy a implementation of a Debug Adapter Protocol for python which is protocol developed my microsoft that defines abstract rules of communication between a IDE or text editor and the debugger. (PDB in our instance) Its a remote debugging application developed by microsoft and used in vscode for debugging. We then do docker run --name dev-container --env-file .env --network host -it -v \u0026lt;local_source_code_path\u0026gt;:\u0026lt;path_inside_container\u0026gt; \u0026lt;image_name\u0026gt; Now that your docker container is running, you can attach your current shell to the container: docker attach \u0026lt;container_name\u0026gt; container name is the name you gave while doing docker run. If you had skipped that part, docker will give you a random name. To get that name, you do docker ps to show currently running container and get the random name from that. Running debugpy inside the container Once you got inside of the shell of the running container, you run the program. python -m debugpy --listen 0.0.0.0:5678 -m flask run --host='0.0.0.0' --port=8080 This is for flask application but if you are running something else modify it accordingly. For more information on debugpy and its options view its github page. Connecting your vscode instance with the debugpy running inside of the container Open vscode in the dir you mounted or the project dir. Make a new launch.json or make vscode make it automaticaly for you. Click on Create a launch.json file \u0026gt; Python Debugger \u0026gt; Remote Attach. Leave the hostname on localhost and port to 5678 because we set it while launching debugpy in previous step. We are keeping hostname to localhost and port to 5678 because we have --network host while running docker container. If 5678 is somehow occupied in your system then forward the port 5678 to some other available port like 9874 in your system by using -p \u0026lt;local_port\u0026gt;:\u0026lt;container_port\u0026gt; while doing docker run. Now you will get something like this in your launch.json 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 { // Use IntelliSense to learn about possible attributes. // Hover to view descriptions of existing attributes. // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387 \u0026#34;version\u0026#34;: \u0026#34;0.2.0\u0026#34;, \u0026#34;configurations\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Python Debugger: Remote Attach\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;debugpy\u0026#34;, \u0026#34;request\u0026#34;: \u0026#34;attach\u0026#34;, \u0026#34;connect\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;localhost\u0026#34;, \u0026#34;port\u0026#34;: 5678 }, \u0026#34;pathMappings\u0026#34;: [ { \u0026#34;localRoot\u0026#34;: \u0026#34;${workspaceFolder}\u0026#34;, \u0026#34;remoteRoot\u0026#34;: \u0026#34;.\u0026#34; } ] } ] } Fix the Path mappings with the paths you had while doing docker run in -v part. I had -v ./app:/app so I will change the pathMapping to something like: 1 2 3 4 5 6 7 8 { \u0026#34;pathMappings\u0026#34;: [ { \u0026#34;localRoot\u0026#34;: \u0026#34;${workspaceFolder}\u0026#34;, \u0026#34;remoteRoot\u0026#34;: \u0026#34;.\u0026#34; } ] } Now while the container and the program you want to debug inside is running with debugpy, you set breakpoints like you normally used to do clicking beside the line numbers. Press F5 to launch the debugging session and try to run the program normally. You can send the request to the server if its http server or something like that. Once the code execution hits breakpoint, your program will pause automatically at the breakpoint and you can inspect variables, invoke functions and others. ","date":"2024-06-22T10:45:58+05:45","permalink":"https://ankitp.com.np/p/debugging-a-python-program-inside-of-a-container/","title":"Debugging a python program inside of a container"}]